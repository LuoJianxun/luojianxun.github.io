{"meta":{"title":"黯居","subtitle":"深居闹市 不显锋芒","description":"个人随笔","author":"黯","url":"https://luojianxun.github.io","root":"/"},"pages":[{"title":"about","date":"2019-07-17T12:51:36.000Z","updated":"2019-07-17T13:52:28.000Z","comments":true,"path":"about/index.html","permalink":"https://luojianxun.github.io/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-07-17T13:43:41.000Z","updated":"2019-07-17T13:53:10.000Z","comments":true,"path":"tags/index.html","permalink":"https://luojianxun.github.io/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-07-17T13:48:30.000Z","updated":"2019-07-17T13:53:38.000Z","comments":true,"path":"categories/index.html","permalink":"https://luojianxun.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"对CTC的一个直观理解","slug":"深度学习/对CTC的一个直观理解","date":"2019-07-17T14:47:23.000Z","updated":"2019-09-16T03:40:03.751Z","comments":true,"path":"2019/07/17/shen-du-xue-xi/dui-ctc-de-yi-ge-zhi-guan-li-jie/","link":"","permalink":"https://luojianxun.github.io/2019/07/17/shen-du-xue-xi/dui-ctc-de-yi-ge-zhi-guan-li-jie/","excerpt":"","text":"翻译自：https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c 如果想要让计算机识别文本，神经网络（neural networks, 以下简称NN）是一个非常好的选择，因为目前来说，它相对于其他方法表现的更好。在实际用例中，NN一般由卷积层和循环层组成，卷积层用来提取序列特征，循环层通过该序列传播信息。最后会以一个简化的矩阵的形式输出每个序列元素对应的字符分数。现在，有两件事需要这个矩阵来做：1.训练：计算训练神经网络过程中的损失值2.推断：通过解码矩阵来获得输入图片包含的信息这两项任务都可以通过CTC操作来实现，下面图片1是一个手写识别系统的概览 来看看CTC的具体操作，本文会在不涉及它背后复杂公式的情况下，挖掘一下到底它是怎么工作的。最后，如果你对相关的python代码和公式有兴趣，我会给出参考链接。 我们为什么想要使用CTC当然，我们可以创建一个包含文本行的图片数据集，然后为图片每个水平位置指定想应的字符，就像下图展示的那样。然后，我们就可以训练一个神经网络来输出每个水平位置上的字符得分。然而，这种简单的解决办法有两个问题需要处理： 1.在字符水平上标注一个数据集是非常耗时（和无聊）的 2.这种办法只能得到字符的得分，因此我们还需要做更多的处理才能从中获得最终的文本。单个字符可能会横跨多个水平位置，例如，我们可能会从下图中得到一个字符串”ttooo”，因为字母”o”在这个地方显得非常宽。这时我们就不得不去除所有多余的”t”和”o”。但是，如果我们要识别的文本是”too”时，去除所有重复的”o”显然会产生一个错误的结果，这种情况该如何处理？ CTC为我们解决了下面的两个问题： 1.我们只需要告诉CTC损失函数图片中包含文本。因此我们可以忽略图片中字符的位置和宽度。 2.不需要为了识别文本而去做更多的处理。 CTC是如何生效的正如之前讨论的那样，我们不想对图片的每个水平位置做标注（一般称为时间步长，下同）。CTC损失函数会引导神经网络的训练过程。我们只会把神经网络的输出矩阵和对应的标注（ground-truth，GT）文本提供给CTC损失函数。但是它是怎样知道每个字符在图片中的位置的？事实上，它并不知道这一信息。取而代之的是，它会尝试所有可能的图片和GT文本的比对情况，并为所有得分求和。这样，如果比对情况的得分很高，则对应的GT文本的得分也会很高。 编码文本如何编码重复字符是我们需要处理的一个问题（你一定还记得我们之前说过的关于单词”too”的问题）。这里是通过引入一个伪字符（一般称其为空，但不要把它当做真正的空，例如，它可以是一个空白字符）。在下文中，我们把这个特殊的字符记为”-“。我们会使用一种巧妙的编码方式来解决重复字符的问题：当编码一个文本时，我们可以插入任意多的空白字符在任意位置，在解码时我们会去除它。不过，我们必须在重复字符之间插入一个空白字符，例如”hello”中的两个 l 之间。此外，我们可以随心所欲地重复每个字符。 让我们来看几个例子： “to” -&gt; “—ttttttooo”，或者”-t-o”，或者”to” “too” -&gt; “—ttttto-o”，或者”-t-o-o-“，或者”to-o”，但不能是”too” 正如你所见的，这种模式也允许我们轻松地为同一个文本创建不同的比对情况，例如，”t-o”、”too”和”-to”都表示同一文本(“to”)，但是对应不同的图片比对情况。我们会训练神经网络以输出一个编码过的文本（通过编码神经网络的输出矩阵）。 计算损失我们需要在训练神经网络时计算给定的图片和对应GT文本的损失值。你已经知道神经网络会输出一个矩阵，这个矩阵的内容是每个字符在每个时间步长上的得分。下图是一个极简版本的矩阵：只有两个时间步长（t0，t1）和三个字符（”a”、”b”和空白字符”-“）。每个时间步长上的字符得分总和为1。 此外，你已经知道损失值是通过将所有可能的GT文本的比对情况的得分求和得到的，这种方法并不关心文本在图片的哪个位置出现。 某个比对情况（或者称为路径，在很多文献中都这样叫它）的得分是将对应字符的得分相乘计算出来的。在上面的例子中，路径”aa”的得分是0.40.4=0.16，”a-“的得分是0.40.6=0.24，”-a”的得分是0.60.4=0.24。为了得到给定GT文本的得分，我们将对应该文本的全部路径得分求和。假设示例中的GT文本是”a”：我们需要计算所有长度为2的可能情况（因为矩阵有两个时间步长），这些情况包括”aa”、”a-“和”-a”，我们前面已经计算了这几条路径的得分，所以我们只需要将它们加起来0.40.4+0.40.6+0.60.4=0.64。如果假设GT文本是””，可以发现只有一条对应的路径”–”，它的得分为0.6*0.6=0.36。 如果你观察的够仔细，可以发现我们计算了GT文本的概率，而不是损失值。然而，损失值只是概率的负对数。损失值通过神经网络反向传播，神经网络的参数会根据使用的优化器进行更新，这里我们不做深入讨论。 解码当我们有一个训练好的模型，我们通常希望用它来识别以前没有用过的图片中的文本。用更加技术性的语言来说，我们想要在给定神经网络的输出矩阵的情况下计算出最可能的文本。你已经知道一种计算给定文本得分的方法。但是这次，我们没有给出任何文本，事实上，这个文本正是我们想要找出的。如果只有少量得到时间步长和字符，尝试每种可能的文本将会起作用，但是在实际使用中，这并不适用。 最优解码路径是一个既简单又高效的算法，它由两步组成： 1.通过选取每个时间步长上最可能的字符来计算最优路径； 2.通过先去除路径中的重复字符，再去除所有空白字符的方法来撤销编码，剩下的内容即识别出的文本。 下图中给出了一个示例，包括”a”、”b”和空白字符”-“三个字符，5个时间步长。将最优路径解码器应用于这个矩阵上：t0时刻最可能的字符是”a”，t1和t2时刻也是”a”，t3时刻空白字符得分最高，最后，t4时刻最可能的是”b”。由此我们获得了一条路径”aaa-b”。去除重复字符，生成字符串”a-b”，去除空白字符，得到文本”ab”，也就是识别出的文本。 当然，最优解码路径也只是一种近似情况。构造一个反例非常容易：如果解码图片3中的矩阵，最终的识别文本将会是””。但我们已经知道””的概率是0.36，而”a”的概率是0.64。不过，在实际情况中，近似算法经常会给出一个较好的结果。另外，还有许多更好的解码器，例如波束搜索解码，前缀搜索解码或令牌传递，它们使用语言结构信息来提升识别结果。 结论、更深入的文献首先，我们聚焦于普通解决方案遇到的问题。然后我们观察了CTC如何解决这些问题。最后，我们了解了CTC怎样编码文本，怎样计算损失以及如何解码CTC训练的神经网络的输出矩阵，从而解释了CTC的工作原理。 这应该让你能够在TensorFlow中使用ctc_loss或者ctc_greedy_decoder时，有一个更直观的理解。但是，如果你想自己实现一个CTC，你需要学习更多的细节，尤其是如何使其运行得更快这点。Graves et al. [1]介绍了CTC操作，这篇文章同时展示了所有相关的数学知识。如果你对如何提升解码感兴趣，可以看看关于波束搜索解码的这几篇文章[2][3]。我使用Python和C++实现了一些解码器和损失函数，你可以在GitHub上找到它[4][5]。最后，如果你想了解如何识别（手写）文本的大图，可以看看我的关于如何建立一个手写文本识别系统的文章。[1] Original paper containing all the math[2] Vanilla beam search decoding[3] Word beam search decoding[4] Python implementation of decoders[5] Implementation of word beam search decoding[6] Text recognition system 附CTC相关的一个博客，写的非常详细：https://xiaodu.io/ctc-explained/","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://luojianxun.github.io/categories/深度学习/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://luojianxun.github.io/tags/DeepLearning/"},{"name":"NLP","slug":"NLP","permalink":"https://luojianxun.github.io/tags/NLP/"},{"name":"RNN","slug":"RNN","permalink":"https://luojianxun.github.io/tags/RNN/"},{"name":"CTC","slug":"CTC","permalink":"https://luojianxun.github.io/tags/CTC/"}]},{"title":"关于Tensor","slug":"深度学习/关于Tensor","date":"2019-07-17T14:30:55.000Z","updated":"2019-07-18T14:55:16.000Z","comments":true,"path":"2019/07/17/shen-du-xue-xi/guan-yu-tensor/","link":"","permalink":"https://luojianxun.github.io/2019/07/17/shen-du-xue-xi/guan-yu-tensor/","excerpt":"","text":"TensorFlow！Tensor？Flow？只要接触过深度学习，必然听说过TensorFlow，众所周知，它是谷歌推出的一款深度学习框架，但是为什么叫TensorFlow？什么是Tensor？它如何Flow呢？ 本文重点讲Tensor，对于Flow，不做深入讨论。 Tensor 定义Tensor，中文译为张量，在深度学习中，0D张量称为标量，1D张量称为向量，2D张量称为矩阵，当然，还有3D张量、4D……更高维。 对于张量，一般有三个关键属性： 轴的个数(维度)。例如，3D张量有3个轴，矩阵有2个轴。在numpy中也叫张量的ndim； 形状(shape)。一般是一个整数元组，表示张量沿每个轴的维度大小（元素个数），向量的形状只包含一个元素，如(5, )，而标量的形状为空，即()； 数据类型(dtype)。张量中包含的数据的类型，float32，uint8等。 注：有一点需要注意，张量tensor和numpy数组不是一个东西！二者可以相互转换，但不是一个东西！ Keras中的TensorKeras也好，TensorFlow也好，都会频繁使用tensor，由于Keras一般是使用tensorflow作为backend，所以二者的tensor是一致的。 以Keras为例（下面代码也全是基于Keras），在训练模型时，很多时候模型的中间层传输的是Tensor，如果直接用print函数打印，得到的可能是这样的内容： Tensor(\"dense_last_1/Relu:0\", shape=(?, 1), dtype=float32) 这显然不是我们想看的东西，换句话说，我们需要的是这个tensor的具体数值，而这里输出的是这个tensor的信息，事实上，当我们创建一个tensor后，直接print输出的也是类似这样的信息： import keras.backend as K import numpy as np # 创建tensor x = [1, 2, 3, 4, 5, 6] x = np.array(x) x = K.variable(x) print(x) 输出： C:\\Users\\WYN\\Desktop>python temp.py Using TensorFlow backend. &lt;tf.Variable 'Variable:0' shape=(6,) dtype=float32_ref> 那如何查看tensor具体的值呢？可以使用eval方法： print(K.eval(x)) 输出： C:\\Users\\WYN\\Desktop>python temp.py Using TensorFlow backend. 2019-06-06 16:36:48.235021: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2. 1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn' t compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations. 2019-06-06 16:36:48.235021: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2. 1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn' t compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations. 2019-06-06 16:36:48.236021: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2. 1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn' t compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations. 2019-06-06 16:36:48.236021: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2. 1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn' t compiled to use SSE4.1 instructions, but these are available on your machine a nd could speed up CPU computations. 2019-06-06 16:36:48.237021: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2. 1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn' t compiled to use SSE4.2 instructions, but these are available on your machine a nd could speed up CPU computations. 2019-06-06 16:36:48.238021: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2. 1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn' t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. [ 1. 2. 3. 4. 5. 6.] 最下面那一行，就是这个tensor的值。 在Keras的源码文件backend.py中，对eval()方法做了封装，要在tensorflow中输出一个tensor的值，要比这个麻烦点。 Tensor的一些操作 合并使用K.concatenate()方法，该方法接收两个参数，第一个是tensor列表，第二个是要合并的轴 示例： # x的shape为(1, 6) x = [1, 2, 3, 4, 5, 6] x = np.array(x).reshape((1, -1)) x = K.variable(x) # y的shape为(1, 6) y = [6, 5, 4, 3, 2, 1] y = np.array(y).reshape((1, -1)) y = K.variable(y) z = K.concatenate([x, y], 0) print(z) print(K.eval(z)) z = K.concatenate([x, y], 1) print(z) print(K.eval(z)) 输出： C:\\Users\\WYN\\Desktop>python temp.py Using TensorFlow backend. Tensor(\"concat:0\", shape=(2, 6), dtype=float32) ...... [[ 1. 2. 3. 4. 5. 6.] [ 6. 5. 4. 3. 2. 1.]] Tensor(\"concat_1:0\", shape=(1, 12), dtype=float32) [[ 1. 2. 3. 4. 5. 6. 6. 5. 4. 3. 2. 1.]] 可以看到，x和y都是shape为(1, 6)的Tensor，在shape=0处合并，新的Tensor的shape为(2, 6，在shape=1处合并，新的Tensor的shape为(1, 12)。 判断是否是稀疏的使用K.is_sparse()函数判断一个Tensor是否是稀疏的。 稠密张量和稀疏张量**一般的张量形式跟矩阵差不多，比如上面的shape为(2, 6)的Tensor，其实跟一个2行6列的矩阵形式基本一致，这种Tnesor也可以称之为稠密张量； 与之对应的，还有一种稀疏张量，其形式为SparseTensor(indices, values, dense_shape) indices是一个维度为(n, ndims)的2-D int64张量，指定非零元素的位置。比如indices=[[1,3], [2,4]]表示[1,3]和[2,4]位置的元素为非零元素。n表示非零元素的个数，ndims表示构造的稀疏张量的维数； values是一个维度为(N)的1-D张量，对应indices所指位置的元素值； dense_shape是一个维度为(ndims)的1-D张量，代表稀疏张量的维度。 另外，可以使用K.to_dense()方法将一个稀疏张量转换成一个稠密张量。 注：文中提到的K指的是keras的backend，即 import keras.backend as K 参考Kreas官方文档：https://keras.io/zh/backend/","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://luojianxun.github.io/categories/深度学习/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://luojianxun.github.io/tags/DeepLearning/"},{"name":"Keras","slug":"Keras","permalink":"https://luojianxun.github.io/tags/Keras/"},{"name":"Python","slug":"Python","permalink":"https://luojianxun.github.io/tags/Python/"}]},{"title":"卷积核替换的一点理解","slug":"深度学习/卷积核替换的一点理解","date":"2019-07-17T13:04:16.000Z","updated":"2019-09-16T03:39:27.109Z","comments":true,"path":"2019/07/17/shen-du-xue-xi/juan-ji-he-ti-huan-de-yi-dian-li-jie/","link":"","permalink":"https://luojianxun.github.io/2019/07/17/shen-du-xue-xi/juan-ji-he-ti-huan-de-yi-dian-li-jie/","excerpt":"","text":"感受野 定义：感受野用来表示网络内部的不同神经元对原图像的感受范围的大小，或者说，cnn每一层输出的特征图(feature map)上的像素点在原始图像上映射的区域大小。 神经元之所以无法对原始图像的所有信息进行感知，是因为在这些网络结构中普遍使用卷积层和池化层，在层与层之间均为局部连接。 神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征；相反，值越小则表示其所包含的特征越趋向局部和细节。因此，感受野的值可以用来大致判断每一层的抽象层次。 卷积核替换理论上来说，当stride为1，padding为0时，可以用2个33的卷积核替换1个55的卷积核，可以用3个33的卷积核替换1个77的卷积核。 在上述条件下，假设原图为nn大小，卷积核为kk大小，步长为s，则卷积后的图像大小计算公式： 比如，原图为2828大小，使用1个55卷积核卷积，得到的特征图大小为(28-5)/1+1=24；使用2个3*3卷积核对齐卷积，第一层特征图大小为(28-3)/1+1=26，第二层特征图的大小为(26-3)/1+1=24，与之前的大小相同。 两个33卷积替换一个55卷积 三个33卷积替换一个77卷积 卷积核替换的优点 从感受野的角度来说，替换的效果是等效的，多层卷积后的一个像素上的感受野与使用单层较大卷积的感受野相同。 从网络结构来说，一般会在每个卷积层后加一层激活函数，通常会使用relu，这样，用多层卷积替换后，会为网络增加更多的非线性，提升网络的泛化性能。 从计算量上来说，多层较小卷积核的计算量会比单层较大卷积核的计算量低很多。以替换77卷积核为例，假设原始图片的通道数为channels，替换前的参数量为77channels=49channels，替换后的参数量为333channels=27channels，减少了45%左右的参数量。 参考：https://www.jianshu.com/p/2b968e7a1715https://blog.csdn.net/bemy1008/article/details/84559905https://www.cnblogs.com/my-love-is-python/p/10514856.html","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://luojianxun.github.io/categories/深度学习/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://luojianxun.github.io/tags/DeepLearning/"},{"name":"CNN","slug":"CNN","permalink":"https://luojianxun.github.io/tags/CNN/"}]},{"title":"Linux 快捷键","slug":"运维/Linux 快捷键","date":"2019-07-17T05:16:03.000Z","updated":"2019-07-18T02:07:30.000Z","comments":true,"path":"2019/07/17/yun-wei/linux-kuai-jie-jian/","link":"","permalink":"https://luojianxun.github.io/2019/07/17/yun-wei/linux-kuai-jie-jian/","excerpt":"","text":"删除 ctrl + d 删除光标所在位置上的字符相当于VIM里x或者dl ctrl + h 删除光标所在位置前的字符相当于VIM里hx或者dh ctrl + k 删除光标后面所有字符相当于VIM里d shift+$ ctrl + u 删除光标前面所有字符相当于VIM里d shift+^ ctrl + w 删除光标前一个单词相当于VIM里db ctrl + y 恢复ctrl+u上次执行时删除的字符 ctrl + ? 撤消前一次输入 alt + r 撤消前一次动作 alt + d 删除光标所在位置的后单词 移动 ctrl + a 将光标移动到命令行开头相当于VIM里shift+^ ctrl + e 将光标移动到命令行结尾处相当于VIM里shift+$ ctrl + f 光标向后移动一个字符相当于VIM里l ctrl + b 光标向前移动一个字符相当于VIM里h ctrl + 方向键左键 光标移动到前一个单词开头 ctrl + 方向键右键 光标移动到后一个单词结尾 ctrl + x 在上次光标所在字符和当前光标所在字符之间跳转 alt + f 跳到光标所在位置单词尾部 替换 ctrl + t 将光标当前字符与前面一个字符替换 alt + t 交换两个光标当前所处位置单词和光标前一个单词 alt + u 把光标当前位置单词变为大写 alt + l 把光标当前位置单词变为小写 alt + c 把光标当前位置单词头一个字母变为大写 ^oldstr^newstr 替换前一次命令中字符串 历史命令编辑 ctrl + p 返回上一次输入命令字符 ctrl + r 输入单词搜索历史命令 alt + p 输入字符查找与字符相接近的历史命令 alt + &gt; 返回上一次执行命令 其它 ctrl + s 锁住终端 ctrl + q 解锁终端 ctrl + l 清屏相当于命令clear ctrl + c 另起一行 ctrl + i 类似TAB健补全功能 ctrl + o 重复执行命令 alt + 数字键 操作的次数","categories":[{"name":"运维","slug":"运维","permalink":"https://luojianxun.github.io/categories/运维/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://luojianxun.github.io/tags/Linux/"}]},{"title":"HDF5文件","slug":"开发/HDF5文件","date":"2019-07-16T14:30:16.000Z","updated":"2019-09-16T03:43:24.370Z","comments":true,"path":"2019/07/16/kai-fa/hdf5-wen-jian/","link":"","permalink":"https://luojianxun.github.io/2019/07/16/kai-fa/hdf5-wen-jian/","excerpt":"","text":"HDF5文件简介HDF（Hierarchical Data Format）指一种为存储和处理大容量科学数据设计的文件格式及相应库文件。HDF 最早由美国国家超级计算应用中心 NCSA 开发，目前在非盈利组织 HDF 小组维护下继续发展。当前流行的版本是 HDF5。HDF5 拥有一系列的优异特性，使其特别适合进行大量科学数据的存储和操作，如它支持非常多的数据类型，灵活，通用，跨平台，可扩展，高效的 I/O 性能，支持几乎无限量（高达 EB）的单文件存储等。详见其官方介绍：https://support.hdfgroup.org/HDF5/ HDF5文件构成一个HDF5文件就是一个由两种基本数据对象（groups and datasets）存放多种科学数据的容器。 HDF5 group: 包含0个或多个HDF5对象以及支持元数据（metadata）的一个群组结构 HDF5 dataset: 数据元素的一个多维数组以及支持元数据（metadata） 对于group和dataset，最直观的理解，可以参考我们的文件管理系统，不同的文件位于不同的目录下: 目录就是hdf5中的group, 描述了数据集dataset的分类信息，通过group 有效的将多种dataset 进行管理和区分； 文件就是hdf5中的dataset, 表示的是具体的数据。 而对于每一个dataset 而言，除了数据本身之外，这个数据集还会有很多的属性 attribute，在hdf5中，还同时支持存储数据集对应的属性信息，所有的属性信息的集合就叫做metadata。 使用Python操作HDF5文件 安装相关包需要使用h5py模块 pip install h5py 读写文件hdf5文件的读写与Python中操作其他文件类似，通过实例化h5py的File对象，传入不同的mode来达到读写的目的，h5py中支持的mode有以下几种： 模式 解释 r 只读，文件必须存在 r+ 读/写，文件必须存在 w 创建文件，如果文件存在，会清空其内容 w- 或 x 创建文件，如果文件存在，方法报错 a 如果文件存在，对其读/写，否则创建文件 写入hdf5文件使用w模式新建一个hdf5文件，然后可以使用create_group()，create_dataset()方法来创建group和dataset，在创建dataset时需要注意为dataset添加(name, shape, dtype)这三个属性，最后使用类似字典赋值的方式给数据赋值即可。 示例代码： import h5py import numpy as np with h5py.File(\"mytestfile.hdf5\", \"w\") as f: # 创建group grp = f.create_group(\"subgroup\") # 给文件创建dataset dset = f.create_dataset(\"mydataset\", (100,), dtype='i') # 在group里创建dataset dset1 = grp.create_dataset(\"mydataset1\", (50,), dtype='i') dset2 = grp.create_dataset(\"mydataset2\", (10,), dtype='f') # 写入数据 dset[...] = np.arange(100) 读取hdf5文件使用r模式读取hdf5文件，然后调用各种属性即可…… 示例代码： with h5py.File(\"mytestfile.hdf5\", \"r\") as f: # 列出所有group keys = list(f.keys()) print(keys) # 使用键值访问数据。类似字典 dset = f['mydataset'] # 输出数据的属性、值 print(dset.name) print(dset.shape) print(dset.dtype) print(dset[0]) print(dset.attrs[\"temperature\"]) 输出： C:\\Users\\WYN\\Desktop>python h5py_practice.py ['mydataset', 'subgroup'] /mydataset (100,) int32 0 99.5 总结：**这里只列出简单的读写hdf5文件的方法，关于该文件，还有一些其他的用法，比如文件驱动等，如有兴趣，可以参考Python官方文档，文档链接在下面“参考”中给出。 参考博客：https://www.cnblogs.com/xudongliang/p/6907733.htmlhttps://blog.csdn.net/zkp_987/article/details/79852236 Python官方文档：http://docs.h5py.org/en/latest/index.html 附件代码及文件：HDF5.zip","categories":[{"name":"开发","slug":"开发","permalink":"https://luojianxun.github.io/categories/开发/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://luojianxun.github.io/tags/Python/"},{"name":"Data","slug":"Data","permalink":"https://luojianxun.github.io/tags/Data/"}]},{"title":"Python logging","slug":"开发/Python logging","date":"2019-07-16T08:29:03.000Z","updated":"2019-09-16T03:42:17.935Z","comments":true,"path":"2019/07/16/kai-fa/python-logging/","link":"","permalink":"https://luojianxun.github.io/2019/07/16/kai-fa/python-logging/","excerpt":"","text":"1. 基础用法 1.1 日志级别Python 标准库 logging 用作记录日志，默认分为六种日志级别，如下表所示： 表1 Python日志级别及对应数值 日志级别 数值 CRITICAL 50 ERROR 40 WARNING 30 INFO 20 DEBUG 10 NOTSET 0 ### 1.2 Python日志工作流程 官方的Python logging 模块工作的流程图： 整个日志流程中的对象： Logger：即 Logger Main Class，是我们进行日志记录时创建的对象，我们可以调用它的方法传入日志模板和信息，来生成一条条日志记录，称作 Log Record。 Log Record：就代指生成的一条条日志记录。 Handler：即用来处理日志记录的类，它可以将 Log Record 输出到我们指定的日志位置和存储形式等，如我们可以指定将日志通过 FTP 协议记录到远程的服务器上，Handler 就会帮我们完成这些事情。 Formatter：实际上生成的 Log Record 也是一个个对象，那么我们想要把它们保存成一条条我们想要的日志文本的话，就需要有一个格式化的过程，那么这个过程就由 Formatter 来完成，返回的就是日志字符串，然后传回给 Handler 来处理。 Filter：另外保存日志的时候我们可能不需要全部保存，我们可能只需要保存我们想要的部分就可以了，所以保存前还需要进行一下过滤，留下我们想要的日志，如只保存某个级别的日志，或只保存包含某个关键字的日志等，那么这个过滤过程就交给 Filter 来完成。 Parent Handler：Handler 之间可以存在分层关系，以使得不同 Handler 之间共享相同功能的代码。 上述日志工作流程： 判断 Logger 对象对于设置的级别是否可用，如果可用，则往下执行，否则，流程结束。 创建 LogRecord 对象，如果注册到 Logger 对象中的 Filter 对象过滤后返回 False，则不记录日志，流程结束，否则，则向下执行。 LogRecord 对象将 Handler 对象传入当前的 Logger 对象，（图中的子流程）如果 Handler 对象的日志级别大于设置的日志级别，再判断注册到 Handler 对象中的 Filter 对象过滤后是否返回 True 而放行输出日志信息，否则不放行，流程结束。 如果传入的 Handler 大于 Logger 中设置的级别，也即 Handler 有效，则往下执行，否则，流程结束。 判断这个 Logger 对象是否还有父 Logger 对象，如果没有（代表当前 Logger 对象是最顶层的 Logger 对象 root Logger），流程结束。否则将 Logger 对象设置为它的父 Logger 对象，重复上面的 3、4 两步，输出父类 Logger 对象中的日志输出，直到是 root Logger 为止。 以上就是整个 logging 模块的基本架构、对象功能和流程。 1.3 日志输出格式日志的输出格式可以认为设置，默认格式为下图所示： 1.4 基本用法 1.4.1 示例以代码切入，下面是一个简单的例子： import logging logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s ' '- %(levelname)s - %(message)s') logger = logging.getLogger(__name__) logger.info('This is a log info') logger.debug('Debugging') logger.warning('Warning exists') logger.info('Finish') 代码详解： 首先引入了 logging 模块，然后进行了一下基本的配置，这里通过 basicConfig 配置了 level 信息和 format 信息，这里 level 配置为 INFO 信息，即只输出 INFO 级别的信息，另外这里指定了 format 格式的字符串，包括 asctime、name、levelname、message 四个内容，分别代表运行时间、模块名称、日志级别、日志内容，这样输出内容便是这四者组合而成的内容了，这就是 logging 的全局配置。 接下来声明了一个 Logger 对象，它就是日志输出的主类，调用对象的 info() 方法就可以输出 INFO 级别的日志信息，调用 debug() 方法就可以输出 DEBUG 级别的日志信息，非常方便。在初始化的时候我们传入了模块的名称，这里直接使用 name 来代替了，就是模块的名称，如果直接运行这个脚本的话就是 main，如果是 import 的模块的话就是被引入模块的名称，这个变量在不同的模块中的名字是不同的，所以一般使用 name 来表示就好了，再接下来输出了四条日志信息，其中有两条 INFO、一条 WARNING、一条 DEBUG 信息，看下输出结果： 2019-06-28 21:29:36,908 - __main__ - INFO - This is a log info 2019-06-28 21:29:36,908 - __main__ - WARNING - Warning exists 2019-06-28 21:29:36,909 - __main__ - INFO - Finish 可以看到输出结果一共有三条日志信息，每条日志都是对应了指定的格式化内容，另外我们发现 DEBUG 的信息是没有输出的，这是因为我们在全局配置的时候设置了输出为 INFO 级别，所以 DEBUG 级别的信息就被过滤掉了。 1.4.2 用法logging 使用非常简单，使用 basicConfig() 方法就能满足基本的使用需要，如果方法没有传入参数，会根据默认的配置创建Logger 对象，默认的日志级别被设置为 WARNING，默认的日志输出格式如图2，该函数可选的参数如下表所示：表2 BasicConfig()参数列表 参数名称 参数描述 filename 日志输出到文件的文件名，如果指定了这个信息之后，实际上会启用 FileHandler，而不再是 StreamHandler，这样日志信息便会输出到文件中了。 filemode 文件模式，r[+]、w[+]（清除后写入），a[+]（追加写入） format 日志输出的格式 datefat 日志附带日期时间的格式 style 格式占位符，默认为 “%” 和 “{}”，如果 format 参数指定了，这个参数就可以指定格式化时的占位符风格，如 %、{、$ 等 level 设置日志输出级别 stream 定义输出流，用来初始化 StreamHandler 对象，不能 filename 参数一起使用，否则会ValueError 异常 handles 定义处理器，用来创建 Handler 对象，不能和 filename 、stream 参数一起使用，否则也会抛出 ValueError 异常，必须是可迭代的 再来一个例子： import logging logging.basicConfig(level=logging.DEBUG, filename='output.log', datefmt='%Y/%m/%d %H:%M:%S', format='%(asctime)s - %(name)s ' '- %(levelname)s - %(lineno)d ' '- %(module)s - %(message)s') logger = logging.getLogger(__name__) logger.info('This is a log info') logger.debug('Debugging') logger.warning('Warning exists') logger.info('Finish') 这段代码会把日志输出到指定的output.log文件中，文件内容如下： 2019/06/29 09:01:01 - __main__ - INFO - 9 - logging_practice - This is a log info 2019/06/29 09:01:01 - __main__ - DEBUG - 10 - logging_practice - Debugging 2019/06/29 09:01:01 - __main__ - WARNING - 11 - logging_practice - Warning exists 2019/06/29 09:01:01 - __main__ - INFO - 12 - logging_practice - Finish 输出的 format 格式增加了 lineno、module 这两个信息，可以看到日志文件中，同时输出了行号、模块名称等信息。 format常用参数： %(levelno)s：打印日志级别的数值。 %(levelname)s：打印日志级别的名称。 %(pathname)s：打印当前执行程序的路径，其实就是sys.argv[0]。 %(filename)s：打印当前执行程序名。 %(funcName)s：打印日志的当前函数。 %(lineno)d：打印日志的当前行号。 %(asctime)s：打印日志的时间。 %(thread)d：打印线程ID。 %(threadName)s：打印线程名称。 %(process)d：打印进程ID。 %(processName)s：打印线程名称。 %(module)s：打印模块名称。 %(message)s：打印日志信息。 这里只列出一部分参数，具体可以参考官方文档。 1.4.3 记录异常当发生异常时，直接使用无参数的 debug()、info()、warning()、error()、critical() 方法并不能记录异常信息，需要设置 exc_info 参数为 True 才可以，或者使用 exception() 方法，还可以使用 log() 方法，但还要设置日志级别和 exc_info 参数。 示例代码： import logging logging.basicConfig(level=logging.DEBUG, filename='output.log', datefmt='%Y/%m/%d %H:%M:%S', format='%(asctime)s - %(name)s ' '- %(levelname)s - %(lineno)d ' '- %(module)s - %(message)s') logger = logging.getLogger(__name__) try: temp = 2 / 0 except BaseException as e: logger.exception(\"Exception occurred\") logger.error(\"Exception occurred\", exc_info=True) logger.log(level=logging.DEBUG, msg=\"Exception occurred\", exc_info=True) 三种方式的输出是一样的： 2019/07/15 16:32:52 - root - ERROR - 12 - logging_2 - Exception occurred Traceback (most recent call last): File \"C:\\Users\\WYN\\Desktop\\logging_2.py\", line 10, in &lt;module> temp = 2 / 0 ZeroDivisionError: division by zero 2019/07/15 16:32:52 - root - ERROR - 13 - logging_2 - Exception occurred Traceback (most recent call last): File \"C:\\Users\\WYN\\Desktop\\logging_2.py\", line 10, in &lt;module> temp = 2 / 0 ZeroDivisionError: division by zero 2019/07/15 16:32:52 - root - DEBUG - 14 - logging_2 - Exception occurred Traceback (most recent call last): File \"C:\\Users\\WYN\\Desktop\\logging_2.py\", line 10, in &lt;module> temp = 2 / 0 ZeroDivisionError: division by zero 可以从输出的行号中看出是三种记录异常的方法虽然代码不一样，但是输出是一样的，推荐使用第一种(exception) 注：虽然使用logger.info(msg)也可以记录到异常信息，但是，这样只能知道有异常发生，却不知道异常发生在哪，而使用上面的方法，可以得到具体的TraceBack信息。 logger.info(msg)的示例： try: temp = 2 / 0 except BaseException as e: logger.info(e) 输出： 2019/07/15 16:42:48 - __main__ - INFO - 15 - logging_2 - division by zero 2. Handler和Formatter 2.1 Handler日志处理类，一般使用其子类来对日志做对应的处理。 看一个例子： import logging logger = logging.getLogger(__name__) logger.setLevel(level=logging.INFO) handler = logging.FileHandler('output.log') logger.addHandler(handler) logger.info('This is a log info') logger.debug('Debugging') logger.warning('Warning exists') logger.info('Finish') 可以看到，这里没有再使用basicConfig全局配置，而是先声明了一个Logger对象，然后指定了其对应的Handler为FileHandler对象，然后给Logger对象添加对应的Handler即可，最后可以发现日志就会被输出到output.log中，内容如下： This is a log info Warning exists Finish logging模块的Handler有许多种，下面列举一些： StreamHandler：日志输出到流，可以是 sys.stderr，sys.stdout 或者文件。 FileHandler：日志输出到文件。 BaseRotatingHandler：基本的日志回滚方式。 RotatingHandler：日志回滚方式，支持日志文件最大数量和日志文件回滚。 TimeRotatingHandler：日志回滚方式，在一定时间区域内回滚日志文件。 SocketHandler：远程输出日志到TCP/IP sockets。 DatagramHandler：远程输出日志到UDP sockets。 SMTPHandler：远程输出日志到邮件地址。 SysLogHandler：日志输出到syslog。 NTEventLogHandler：远程输出日志到Windows NT/2000/XP的事件日志。 MemoryHandler：日志输出到内存中的指定buffer。 HTTPHandler：通过”GET”或者”POST”远程输出到HTTP服务器。 更多的信息可以参考官方文档：logging.handlers 另外，可以设置多个Handler，每个 Handler 还可以设置 level 信息，最终输出结果的 level 信息会取 Logger 对象的 level 和 Handler 对象的 level 的交集，默认的level为0，也即 NOTSET。 2.2 Formatter格式类，一般用来指定日志格式 示例： import logging logger = logging.getLogger(__name__) logger.setLevel(level=logging.WARN) formatter = logging.Formatter(fmt='%(asctime)s - %(name)s ' '- %(levelname)s - %(message)s', datefmt='%Y/%m/%d %H:%M:%S') handler = logging.StreamHandler() handler.setFormatter(formatter) logger.addHandler(handler) logger.debug('Debugging') logger.critical('Critical Something') logger.error('Error Occurred') logger.warning('Warning exists') logger.info('Finished') 输出： 2019/07/15 17:40:43 - __main__ - CRITICAL - Critical Something 2019/07/15 17:40:43 - __main__ - ERROR - Error Occurred 2019/07/15 17:40:43 - __main__ - WARNING - Warning exists Formatter对象常用变量格式如下表所示：表3 Formatter对象常用变量格式 变量 格式 变量描述 asctime %(asctime)s 将日志的时间构造成可读的形式，默认情况下是精确到毫秒，如 2018-10-13 23:24:57,832，可以额外指定 datefmt 参数来指定该变量的格式 name %(name) 日志对象的名称 filename %(filename)s 不包含路径的文件名 pathname %(pathname)s 包含路径的文件名 funcName %(funcName)s 日志记录所在的函数名 levelname %(levelname)s 日志的级别名称 message %(message)s 具体的日志信息 lineno %(lineno)d 日志记录所在的行号 pathname %(pathname)s 完整路径 process %(process)d 当前进程ID processName %(processName)s 当前进程名称 thread %(thread)d 当前线程ID threadName %threadName)s 当前线程名称 其他格式可以参考官方文档：LogRecord attributes 3. 自定义Logger 3.1 用法前面的介绍可以满足基本的日志需求，但是如果有其他需要，也可以自定义Logger。自定义日志级别时注意尽量不要和默认的日志级别数值相同。自定义Logger需要使用前文中的Handler和Fomatter。 一个系统只有一个根 Logger 对象，并且该对象不能被直接实例化，获取 Logger 对象的方法为 getLogger。 可以创造多个 Logger 对象，但是真正输出日志的是根 Logger 对象。每个 Logger 对象都可以设置一个名字，如设置logger = logging.getLogger(name)，name 是 Python 中的一个特殊内置变量，他代表当前模块的名称（默认为 main）。Logger 对象的 name 建议使用使用以点号作为分隔符的命名空间等级制度。 Logger 对象可以设置多个 Handler 对象和 Filter 对象，Handler 对象又可以设置 Formatter 对象。 Logger 对象和 Handler 对象都可以设置级别，而默认 Logger 对象级别为 30 ，也即 WARNING，默认 Handler 对象级别为 0，也即 NOTSET。 这样设计是为了更好的灵活性，比如有时候需要在控制台中输出DEBUG 级别的日志，同时在文件中输出WARNING级别的日志。可以只设置一个最低级别的 Logger 对象，两个不同级别的 Handler 对象。 示例： import logging logger = logging.getLogger(\"logger\") handler1 = logging.StreamHandler() handler2 = logging.FileHandler(filename=\"output.log\") logger.setLevel(logging.DEBUG) handler1.setLevel(logging.WARNING) handler2.setLevel(logging.DEBUG) formatter = logging.Formatter(\"%(asctime)s %(name)s ' '%(levelname)s %(message)s\") handler1.setFormatter(formatter) handler2.setFormatter(formatter) logger.addHandler(handler1) logger.addHandler(handler2) logger.debug('This is a customer debug message') logger.info('This is a customer info message') logger.warning('This is a customer warning message') logger.error('This is a customer error message') logger.critical('This is a customer critical message') 控制台输出： 2019-07-15 19:14:12,546 logger WARNING This is a customer warning message 2019-07-15 19:14:12,546 logger ERROR This is a customer error message 2019-07-15 19:14:12,546 logger CRITICAL This is a customer critical message output.log文件内容： 2019-07-15 19:14:12,545 logger DEBUG This is a customer debug message 2019-07-15 19:14:12,546 logger INFO This is a customer info message 2019-07-15 19:14:12,546 logger WARNING This is a customer warning message 2019-07-15 19:14:12,546 logger ERROR This is a customer error message 2019-07-15 19:14:12,546 logger CRITICAL This is a customer critical message 3.2 注意事项有几点需要注意，首先，设置了Logger对象后，就不用使用logging的方法输出日志了，因为logging的方法使用的是默认配置的Logger对象，否则会重复输出日志。 示例： import logging logger = logging.getLogger(\"logger\") handler = logging.StreamHandler() handler.setLevel(logging.DEBUG) formatter = logging.Formatter(\"%(asctime)s %(name)s \" \"%(levelname)s %(message)s\") handler.setFormatter(formatter) logger.addHandler(handler) logger.debug('This is a customer debug message') logging.info('This is an customer info message') logger.warning('This is a customer warning message') logger.error('This is an customer error message') logger.critical('This is a customer critical message') 可以看到，代码中，使用了一句logging.info，最后输出的日志中，会有重复。 输出： 2019-07-15 20:06:53,816 logger WARNING This is a customer warning message WARNING:logger:This is a customer warning message 2019-07-15 20:06:53,817 logger ERROR This is a customer error message ERROR:logger:This is a customer error message 2019-07-15 20:06:53,817 logger CRITICAL This is a customer critical message CRITICAL:logger:This is a customer critical message 每一条日志信息都重复输出两次。 另外！上面的输出，其实还有不正常的地方——代码中设置的level为debug，但是debug和info都没有输出。具体原因，由前文所述，最后输出日志的是根Logger，而根Logger的默认级别为warning，所以debug和info级别的日志没有输出，而且，在这里打印logger的级别print(logger.level)，发现输出为0。 注：这里的原因是个人猜想，没有验证，未必准确。 后续：了解到，使用logging.root可以获取到根logger，于是获取了根logger，root_logger=logging.root，然后使用print(root_logger.level)打印了根logger的级别，发现确实是30，即warning，所以debug和info级别的日志不会输出，个人猜想基本得证。 这就又引出了一个注意点，使用handler时，logger的level也要设置，上面的代码中，在实例化logger的下一行添加一句logger.setLevel(logging.DEBUG)，最后即可输出debug和info信息。 4. Logger配置前文中，日志都是直接配置在代码中的，下面介绍几种文件配置方法。 4.1 通过字典配置将配置信息存入一个字典里，然后从字典中导入配置。字典的格式参考官方文档：Configuration dictionary schema 示例： import logging.config config = { 'version': 1, 'formatters': { 'simple': { 'format': '%(asctime)s - %(name)s - ' '%(levelname)s - %(message)s', }, # 其他的 formatter }, 'handlers': { 'console': { 'class': 'logging.StreamHandler', 'level': 'DEBUG', 'formatter': 'simple' }, 'file': { 'class': 'logging.FileHandler', 'filename': 'logging.log', 'level': 'DEBUG', 'formatter': 'simple' }, # 其他的 handler }, 'loggers':{ 'StreamLogger': { 'handlers': ['console'], 'level': 'DEBUG', }, 'FileLogger': { # 既有 console Handler，还有 file Handler 'handlers': ['console', 'file'], 'level': 'DEBUG', }, # 其他的 Logger } } logging.config.dictConfig(config) StreamLogger = logging.getLogger(\"StreamLogger\") FileLogger = logging.getLogger(\"FileLogger\") StreamLogger.info('This is dict config logger') 输出： 2019-07-15 20:32:57,265 - StreamLogger - INFO - This is dict config logger 4.2 通过ini文件配置将配置信息写入一个ini文件中，logger从该文件中读取配置信息。配置文件的格式参考官方文档：Configuration file format logger_config.ini配置文件内容： [loggers] keys=root,sampleLogger [handlers] keys=consoleHandler [formatters] keys=sampleFormatter [logger_root] level=DEBUG handlers=consoleHandler [logger_sampleLogger] level=DEBUG handlers=consoleHandler qualname=sampleLogger propagate=0 [handler_consoleHandler] class=StreamHandler level=DEBUG formatter=sampleFormatter args=(sys.stdout,) [formatter_sampleFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s代码： import logging.config logging.config.fileConfig(fname='logger_config.ini', disable_existing_loggers=False) logger = logging.getLogger(\"sampleLogger\") logger.info('This is ini config logger') 输出： 2019-07-15 20:38:26,417 - sampleLogger - INFO - This is ini config logger 4.3 通过yaml文件配置将配置信息写入一个ini文件中，logger从该文件中读取配置信息。同样，配置文件的格式参考官方文档：Configuration file format logger_config.yaml配置文件内容： version: 1 formatters: simple: format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s' handlers: console: class: logging.StreamHandler level: DEBUG formatter: simple loggers: simpleExample: level: DEBUG handlers: [console] propagate: no root: level: DEBUG handlers: [console] 代码： import logging.config import yaml with open(\"logger_config.yaml\", \"r\") as f: config = yaml.safe_load(f.read()) logging.config.dictConfig(config) logger = logging.getLogger(\"sampleLogger\") logger.info(\"This is yaml config logger\") 输出： 2019-07-15 20:43:48,936 - sampleLogger - INFO - This is yaml config logger 5. 常见问题 5.1 中文乱码在basicConfig()或handlers中设置编码格式为“utf-8”即可： # 使用默认的 Logger 配置 logging.basicConfig(handlers=[logging.FileHandler( \"test.log\", encoding=\"utf-8\")], level=logging.DEBUG) # 自定义 Logger 配置 handler = logging.FileHandler(filename=\"test.log\", encoding=\"utf-8\") 5.2 临时禁用日志有时需要暂时禁用日志，之后再恢复使用，一般有两种方法可以做到。 一种方法是在使用默认配置时，给 logging.disabled() 方法传入禁用的日志级别，就可以禁止设置级别以下的日志输出了，另一种方法时在自定义 Logger 时，Logger 对象的 disable 属性设为 True，默认值是 False，也即不禁用。 logging.disable(logging.INFO) logger.disabled = True 5.3 日志文件按照时间划分或者按照大小划分如果将日志保存在一个文件中，那么时间一长，或者日志一多，单个日志文件就会很大，既不利于备份，也不利于查看。logging.handlers 文件中提供了 TimedRotatingFileHandler 和 RotatingFileHandler 类分别可以实现按时间和大小划分。 按时间分： # 每隔 1小时 划分一个日志文件，interval 是时间间隔，备份文件为 10 个 handler2 = logging.handlers.TimedRotatingFileHandler(\"test.log\", when=\"H\", interval=1, backupCount=10) 按大小分： # 每隔 1000 Byte 划分一个日志文件，备份文件为 3 个 file_handler = logging.handlers.RotatingFileHandler(\"test.log\", mode=\"w\", maxBytes=1000, backupCount=3, encoding=\"utf-8\") 参考一个个人博客，解释的很清晰（但是内容上有点问题，可能Python版本较老吧）：https://cuiqingcai.com/6080.html 腾讯云社区文章，比较详细（在最后自定义Logger处内容上同样有点问题，上文中有解释）：https://cloud.tencent.com/developer/article/1354396 简书文章，代码、干货：https://www.jianshu.com/p/26849de20a83 设置了handler等级，未设置logger等级，导致最后没有输出级别较低的日志信息的解决办法：https://cloud.tencent.com/developer/ask/62724 官方文档（Python3.7，英文）：https://docs.python.org/3/library/logging.html 附件示例代码、文件：Python_log.zip","categories":[{"name":"开发","slug":"开发","permalink":"https://luojianxun.github.io/categories/开发/"}],"tags":[{"name":"python","slug":"python","permalink":"https://luojianxun.github.io/tags/python/"}]},{"title":"Hello MyBlog","slug":"随笔/hello-blog","date":"2019-07-15T17:59:03.000Z","updated":"2019-07-18T02:08:04.000Z","comments":true,"path":"2019/07/16/sui-bi/hello-blog/","link":"","permalink":"https://luojianxun.github.io/2019/07/16/sui-bi/hello-blog/","excerpt":"","text":"这是我的第一个博客，耗时两个小时，终于搭建起来了…… 作为一个计算机学科的小白，一直想弄一个自己的博客，但是由于技术有限，时间有限，搁置了很久。最近找到了一个不错的博客搭建方法：hexo。有兴趣的可以去查阅相关资料，了解一下，我的博客就是在hexo的基础上搭建起来的。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://luojianxun.github.io/categories/随笔/"}],"tags":[{"name":"daily","slug":"daily","permalink":"https://luojianxun.github.io/tags/daily/"}]},{"title":"Hello World","slug":"随笔/hello-world","date":"2019-07-15T14:37:03.000Z","updated":"2019-07-17T14:53:40.000Z","comments":true,"path":"2019/07/15/sui-bi/hello-world/","link":"","permalink":"https://luojianxun.github.io/2019/07/15/sui-bi/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}